# [深度学习中知识蒸馏研究综述](http://cjc.ict.ac.cn/online/onlinepaper/srr-202286135608.pdf "原文")

计算机学报，2022

华东师范大学计算机科学与技术学院

## 引言

### 深度学习在实际应用中的挑战

1. 学习任务错综复杂->模型的复杂化->参数增加，算力消耗大 ~~嵌入式设备和移动设备~~
2. 监督学习依赖大量人工标签，数据集难获取，大量标注耗费大
3. AI与小型化智能化的设备深度融合需要 **便携、高效、轻量** 的模型

### 知识蒸馏

Hinton于2015提出了知识蒸馏：**利用复杂的深层网络模型向浅层的小型网络模型迁移知识**

: ![图 1](images/深度学习中知识蒸馏研究综述_计算机学报/ee4834898a35d05edf921aa7119572ed0ac9cfde911732b3219393af4c31be0a.png)  

### 知识蒸馏面临的挑战

- 模型问题：教师-学生模型不匹配
- 成本问题：模型试错成本高
- 可解释性不足

## 理论基础及分类

### 理论基础

#### 知识蒸馏定义

知识蒸馏本质上属于迁移学习的范畴，其主要思路是将已训练完善的模型作为教师模型，通过控制 **“温度”** 从模型的输出结果中 **“蒸馏”** 出 **“知识”** 用于学生模型的训练，并希望轻量级的学生模型能够学到教师模型的“知识”，达到和教师模型相同的表现。

- **“知识”**：狭义上的解释是教师模型的输出中包含了某种相似性，广义
上的解释是教师模型能够被利用的一切知识形式（如特征、参数、模块）。
- **“蒸馏”**：指通过某些方法（如控制参数），能够放大这种知识的相似性，并
使其显现的过程。
- **“温度”**：控制输出概率分布的软化程度

![图 2](images/深度学习中知识蒸馏研究综述_计算机学报/569b2fad11b8e7d8bdc42e072e591eb215dfd32a8ca8f6c8436d83b4725d8284.png)

#### 损失函数

![图 4](images/深度学习中知识蒸馏研究综述_计算机学报/cf0408226a18e3a6d6f8d26858614a1c2301ea10d025c9a10e3830e1bbe39f53.png)  

![图 5](images/深度学习中知识蒸馏研究综述_计算机学报/d5ecdcdb0eaeedac88535beca068da15d9b0fe0caef89f3904d6a16f4faa9340.png)  

![图 6](images/深度学习中知识蒸馏研究综述_计算机学报/d4973a6335ea3315d85e8780c5bd605baefef924f09791e70a1583d4c0e9f06f.png)  

### 分类

![图 3](images/深度学习中知识蒸馏研究综述_计算机学报/aa8ebca15ff0c3af0a49bf2d058a09490b4e911def81fa8b70c75faa1f95a172.png)  

## 知识传递形式

![图 7](images/深度学习中知识蒸馏研究综述_计算机学报/8114199c8e81442206efa962a2f768c1ba2944fcde39bb52501709ea5f665ccd.png) 

![图 9](images/深度学习中知识蒸馏研究综述_计算机学报/69dde76b7f0f65adbbce7abebfbbb3684ad6f003ab4ec8671eff9f0fc449b7f3.png)  

### 1.标签知识

### 2. 中间层知识

![图 8](images/深度学习中知识蒸馏研究综述_计算机学报/535cb995dd0d9d37283700539400aa065b87b8937454ba8f1a65c2f10f39329e.png)  

### 3. 参数知识

**教师平均Mean Teacher**：利用加权教师模型上一阶段更新后的参数参与下一阶段的参数更新

**模块注入Module Injection**：直接利用教师模型的参数，将教师的部分网络模块嵌入到学生模型中参与蒸馏训练，这样可以使得学生模型的模块前后处于一种接近教师模型内部的训练环境

### 4. 结构化知识

![图 10](images/深度学习中知识蒸馏研究综述_计算机学报/0821a9315b4156ba6d2ae8434de4cc215b229e518f3f5a93220cbfecb8d8bc72.png)  

### 5. 图表示知识

**图表示学习**：把每个节点映射到一个低维、稠密的向量中，目标是向量之间的关系可以反映节点在图中的关系

使用图表示知识的蒸馏方法主要集中在两类场景：**经典深度神经网络**中提取的图结构化关系表示知识 和 **图神经网络**上的知识蒸馏

![图 11](images/深度学习中知识蒸馏研究综述_计算机学报/d5b318cb6d5874e0f9cb083944f4545b5bebec8b4622e76ce8d5d2ab56cff7c0.png)  


## 学习方式

![图 12](images/深度学习中知识蒸馏研究综述_计算机学报/f27142ce756b2f431cb4ed157f81d68df169c126df13b529e821425a1ef464ce.png)  

![图 13](images/深度学习中知识蒸馏研究综述_计算机学报/784a711faafcd03ebdfe8000118c5f03cdd81f863dc5ab51a51a95474e23f434.png)  

### 离线蒸馏（目前主流）

学生模型基于预训练好的、参数固定的教师模型（往往是预训练完备、高性能的模型）学习

### 在线蒸馏（算力消耗大、对模型的压缩意义不大）

教师和学生模型同时参与训练和参数更新

具有多种学习形式：

- 互学习（Mutual Learing）：将两个或多个学生模型一起训练并将他们的输出知识作为互相之间的学习目标，优势在于模型之间可以相互促进实现互补
- 共享学习：在多个训练模型中通过构建教师模型来收集和汇总知识，并将知识反馈给各个模型，可以用于提升多分类器的效果，也可用于多任务学习、长尾数据训练
- 协同学习（Collaborative Learning）：类似于互学习，在任务上训练多个独立的分支后实现知识集成与迁移并实现学生的同时更新

### 自蒸馏（提升下游任务模型性能的手段、不以模型压缩或迁移学习为目的）

学生模型不依赖于外在模型，而是利用自身信息进行蒸馏，通常是将深层信息回传给浅层指导训练过程

针对问题：传统两阶段蒸馏方法预训练教师模型的耗时、教师学生模型之间的不匹配

### 多模型蒸馏

有多个模型参与，有以下两种：

- 多教师蒸馏：从多个预训练完备的教师模型中提炼知识用于指导学生模型（主要关注分类问题，相对局限）
- 集成学习：多个学生模型同时参与训练，通常采用同构模型，中间层特征利用率高；学生将知识汇总到集成器中，集成器再将知识分发给所有学生

### 零样本Zero-shot/无数据Data-free蒸馏

![图 14](images/深度学习中知识蒸馏研究综述_计算机学报/091fcb1c65a2d8eacebe56e11c63fa0ce6507d16484fbf928a9e14444854074c.png)  

利用预训练完备的教师模型的先验信息（激活层输出的激活信息、批量归一化层的统计量BNS、输出层的信息熵、教师-学生之间的差异度量），在没有任何源训练数据集的情况下进行知识蒸馏并训练学生模型

根据还原等效样本的方法可以分成两类：

- 对抗合成方法：利用生成器与教师模型形成对抗学习来估计样本分布
- 噪声优化方法：直接优化每次输入教师模型的噪声变量直到收敛

### 特权蒸馏

![图 15](images/深度学习中知识蒸馏研究综述_计算机学报/8ad81123104b2e788aececd04bf5e817644cfb71914fcc5c49bc3c9394a975cc.png)  

教师模型享有学生模型无法访问的特权信息

## 学习目的

### 模型压缩（最初目的、主要的应用方向）

### 跨模态/跨领域

![图 16](images/深度学习中知识蒸馏研究综述_计算机学报/5e862373998ae79b273604afa628a16c485414b7b59d6fa6eac7df13b6acdb75.png)  

跨模态：利用同一数据的不同形态来表示学习样本
跨领域：在不同任务上学习并实现知识的迁移（可以是同模态）

### 隐私保护

### 持续学习（Continual Learning）

#### 定义

一个学习系统能够不断地从新样本中学习新的知识，并且保q存大部分已经学习到的知识，**挑战：灾难性遗忘**，需要平衡新知识与旧知识之间的关系

## 交叉领域

### GANs（Generative Adversarial Networks）

应用领域：超分辨率、风格迁移、图像合成

缺点：计算复杂、存储开销大

对GAN模型蒸馏的三类：对生成器蒸馏、对判别器蒸馏、同时对G和D蒸馏

![图 17](images/深度学习中知识蒸馏研究综述_计算机学报/e307b21c393c236cf1c43543c6b4f13358c7968cc2a1bb6939f8a87f3f801499.png)  

知识蒸馏+GAN在模型压缩方面取得了很好效果，挑战：

- 不易训练：
- 不易解释：

### 强化学习

深度强化学习模型训练开销非常大

### 元学习（Meta Learning）

元学习+知识蒸馏的问题：

- 过拟合
- 结构不匹配
- 新旧任务不匹配

### 自动机器学习（Auto ML）

主要技术：神经架构搜索和超参数优化

神经架构搜索+知识蒸馏挑战：

- 结构不匹配
- 搜索空间复杂
- 鲁棒性不足

### 自监督学习（Self-Supervised Learning, SSL）

![图 18](images/深度学习中知识蒸馏研究综述_计算机学报/8f93c3418ffad5a62c4907236c341755ecda4e9fd9ec5a7960af4457650849dd.png)  

## 主要应用

计算机视觉、自然语言处理、推荐系统

## 原理和解释

标签平滑正则化（Label Smooth Regularization, LSR）、知识蒸馏的泛化理论

## 未来发展趋势

- 模型压缩方面缺少针对剪枝和量化的定制化蒸馏方法
- 新领域新场景
- 原理和可解释性（可以从模型不确定性、能量与信息论等角度）
- 新的探索（结合自监督学习和因果推断）
